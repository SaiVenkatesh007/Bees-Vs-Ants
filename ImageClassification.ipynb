{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOPIC - Image classification of Insects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-02 23:40:30.631407: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-02 23:40:30.770459: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-02 23:40:30.770485: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-02 23:40:30.797316: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-12-02 23:40:31.572117: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-02 23:40:31.572198: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-02 23:40:31.572208: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_dir = '../img/BeesAndAnts'\n",
    "test_path = my_data_dir+'/test/'\n",
    "train_path = my_data_dir+'/train/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rescaling datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "path = '../img/BeesAndAnts/train/'\n",
    "file_path = os.listdir(path)\n",
    "\n",
    "\n",
    "for i in file_path:\n",
    "    data = path + i\n",
    "    img = cv2.imread(data)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    image = cv2.resize(img, (224,224))\n",
    "    cv2.imwrite(data,image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../img/BeesAndAnts/test/'\n",
    "file_path = os.listdir(path)\n",
    "\n",
    "for i in file_path:\n",
    "    data = path + i\n",
    "    img = cv2.imread(data)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    image = cv2.resize(img, (224,224))\n",
    "    cv2.imwrite(data,image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "cv2.imread('../img/BeesAndAnts/train/Image_1.jpg').shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rearranging Images in respective class folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Image_1.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Image_2.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Image_3.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Image_4.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Image_5.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      filename label\n",
       "0  Image_1.jpg     0\n",
       "1  Image_2.jpg     1\n",
       "2  Image_3.jpg     1\n",
       "3  Image_4.jpg     0\n",
       "4  Image_5.jpg     1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv = pd.read_csv(\"../img/BeesAndAnts/Training_set.csv\")\n",
    "train_csv.label[train_csv[\"label\"]==\"bees\"] = 0\n",
    "train_csv.label[train_csv[\"label\"]==\"ants\"] = 1\n",
    "train_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_csv.label.unique())):\n",
    "    try:\n",
    "        os.mkdir(train_path+str(i))\n",
    "    except:\n",
    "        pass\n",
    "for i,j in zip(train_csv.filename,train_csv.label):\n",
    "    try:\n",
    "        os.rename(train_path+i,train_path+str(j)+\"/\"+i)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.listdir(test_path)\n",
    "os.mkdir(test_path+\"test\")\n",
    "\n",
    "for i in file_path:\n",
    "    try:\n",
    "        os.rename(test_path+i,test_path+\"test/\"+i)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we are using the image data generator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = (224,224,3)\n",
    "image_gen = ImageDataGenerator(rotation_range=20,\n",
    "                               width_shift_range=0.10,\n",
    "                               height_shift_range=0.10,\n",
    "                               rescale=1/255,\n",
    "                               shear_range=0.1,\n",
    "                               zoom_range=0.1,\n",
    "                               horizontal_flip=True,\n",
    "                               fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a convolutional neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 277 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_image_gen = image_gen.flow_from_directory(train_path,\n",
    "                                               target_size= (224,224),\n",
    "                                                \n",
    "                                               batch_size=batch_size,\n",
    "                                               class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 120 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "test_image_gen = image_gen.flow_from_directory(test_path,\n",
    "                                               target_size= (224,224),\n",
    "                                              \n",
    "                                               batch_size=batch_size,\n",
    "                                               class_mode='categorical',shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 0, '1': 1}\n"
     ]
    }
   ],
   "source": [
    "print(train_image_gen.class_indices)\n",
    "Labels=train_image_gen.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 224, 224, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_image_gen[1][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-02 23:40:34.153453: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/piyush/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2022-12-02 23:40:34.153475: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-12-02 23:40:34.153496: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (piyushLap): /proc/driver/nvidia/version does not exist\n",
      "2022-12-02 23:40:34.153660: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape = (224, 224, 3), include_top = False, weights = \"imagenet\", classes = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = tf.keras.Sequential([base_model,\n",
    "                                 tf.keras.layers.GlobalAveragePooling2D(),\n",
    "                                 tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "                                 tf.keras.layers.Dropout(.2),\n",
    "                                 tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "                                 tf.keras.layers.Dropout(.2),\n",
    "                                 tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "                                 tf.keras.layers.Dropout(.2),\n",
    "                                 tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "                                 tf.keras.layers.Dense(2, activation=\"softmax\")                                     \n",
    "                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "5/5 [==============================] - 8s 1s/step - loss: 0.6849 - accuracy: 0.5307\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.4551 - accuracy: 0.8159\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.2570 - accuracy: 0.9061\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.2035 - accuracy: 0.9134\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.2474 - accuracy: 0.9061\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.1992 - accuracy: 0.9386\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.1931 - accuracy: 0.9278\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.1785 - accuracy: 0.9386\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.1320 - accuracy: 0.9531\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0866 - accuracy: 0.9747\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.1149 - accuracy: 0.9675\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.1152 - accuracy: 0.9603\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.0664 - accuracy: 0.9747\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.0720 - accuracy: 0.9675\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.0632 - accuracy: 0.9747\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.0834 - accuracy: 0.9819\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.0721 - accuracy: 0.9783\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.0845 - accuracy: 0.9639\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.0677 - accuracy: 0.9747\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.0335 - accuracy: 0.9892\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.0192 - accuracy: 0.9964\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.0258 - accuracy: 0.9892\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.0113 - accuracy: 0.9964\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.0706 - accuracy: 0.9783\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.0363 - accuracy: 0.9928\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.0253 - accuracy: 0.9928\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.0491 - accuracy: 0.9783\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.0222 - accuracy: 0.9964\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.0419 - accuracy: 0.9856\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.0234 - accuracy: 0.9892\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.0119 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.0142 - accuracy: 0.9964\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.0268 - accuracy: 0.9892\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.0134 - accuracy: 0.9928\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.0272 - accuracy: 0.9856\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.0343 - accuracy: 0.9928\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.0393 - accuracy: 0.9819\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.0617 - accuracy: 0.9747\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.0364 - accuracy: 0.9856\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.0372 - accuracy: 0.9819\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.0229 - accuracy: 0.9856\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.0196 - accuracy: 0.9928\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.0165 - accuracy: 0.9964\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.0055 - accuracy: 0.9964\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.0237 - accuracy: 0.9928\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.0243 - accuracy: 0.9892\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.0293 - accuracy: 0.9928\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.0227 - accuracy: 0.9928\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.0281 - accuracy: 0.9964\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 6s 1s/step - loss: 0.0120 - accuracy: 0.9964\n"
     ]
    }
   ],
   "source": [
    "base_learning_rate = 0.0001\n",
    "model1.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model1.fit(train_image_gen, epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 6s 990ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,\n",
       "       1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n",
       "       1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "       0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,\n",
       "       0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = np.argmax(model1.predict(train_image_gen), axis = 1)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.46      0.46       142\n",
      "           1       0.44      0.44      0.44       135\n",
      "\n",
      "    accuracy                           0.45       277\n",
      "   macro avg       0.45      0.45      0.45       277\n",
      "weighted avg       0.45      0.45      0.45       277\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(train_image_gen.classes,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.inception_v3 import InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = InceptionV3(input_shape = (224, 224, 3), include_top = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "x = tf.keras.layers.Flatten()(base_model.output)\n",
    "x = tf.keras.layers.Dense(2048, activation='relu')(x)\n",
    "x = tf.keras.layers.Dense(1024, activation='relu')(x)\n",
    "x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "# Add a final sigmoid layer with 1 node for classification output\n",
    "x = tf.keras.layers.Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "model2 = tf.keras.models.Model(base_model.input, x)\n",
    "\n",
    "model2.compile(optimizer = RMSprop(lr=0.0001), loss = 'binary_crossentropy', metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "5/5 [==============================] - 15s 2s/step - loss: 4.3810 - acc: 0.4729\n",
      "Epoch 2/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 1.3040 - acc: 0.7292\n",
      "Epoch 3/60\n",
      "5/5 [==============================] - 12s 3s/step - loss: 0.9919 - acc: 0.6895\n",
      "Epoch 4/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.5167 - acc: 0.8303\n",
      "Epoch 5/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.5913 - acc: 0.7292\n",
      "Epoch 6/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.4524 - acc: 0.8159\n",
      "Epoch 7/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.3909 - acc: 0.8375\n",
      "Epoch 8/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.4157 - acc: 0.8159\n",
      "Epoch 9/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.2263 - acc: 0.9242\n",
      "Epoch 10/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.2987 - acc: 0.8809\n",
      "Epoch 11/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.1374 - acc: 0.9458\n",
      "Epoch 12/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.4208 - acc: 0.8375\n",
      "Epoch 13/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.1258 - acc: 0.9495\n",
      "Epoch 14/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.5611 - acc: 0.8267\n",
      "Epoch 15/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.2035 - acc: 0.9025\n",
      "Epoch 16/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.1056 - acc: 0.9567\n",
      "Epoch 17/60\n",
      "5/5 [==============================] - 12s 3s/step - loss: 0.1213 - acc: 0.9603\n",
      "Epoch 18/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.3041 - acc: 0.8773\n",
      "Epoch 19/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.1950 - acc: 0.9206\n",
      "Epoch 20/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.1564 - acc: 0.9386\n",
      "Epoch 21/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.0758 - acc: 0.9675\n",
      "Epoch 22/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.1232 - acc: 0.9675\n",
      "Epoch 23/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.1376 - acc: 0.9422\n",
      "Epoch 24/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.0251 - acc: 0.9928\n",
      "Epoch 25/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.5363 - acc: 0.8339\n",
      "Epoch 26/60\n",
      "5/5 [==============================] - 12s 3s/step - loss: 0.1563 - acc: 0.9495\n",
      "Epoch 27/60\n",
      "5/5 [==============================] - 11s 2s/step - loss: 0.0225 - acc: 0.9892\n",
      "Epoch 28/60\n",
      "5/5 [==============================] - 11s 2s/step - loss: 0.2844 - acc: 0.9025\n",
      "Epoch 29/60\n",
      "5/5 [==============================] - 11s 2s/step - loss: 0.0946 - acc: 0.9675\n",
      "Epoch 30/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.0997 - acc: 0.9531\n",
      "Epoch 31/60\n",
      "5/5 [==============================] - 11s 2s/step - loss: 0.0261 - acc: 0.9892\n",
      "Epoch 32/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.0595 - acc: 0.9892\n",
      "Epoch 33/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.1039 - acc: 0.9603\n",
      "Epoch 34/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.2896 - acc: 0.8953\n",
      "Epoch 35/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.0992 - acc: 0.9567\n",
      "Epoch 36/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.0338 - acc: 0.9892\n",
      "Epoch 37/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.0453 - acc: 0.9856\n",
      "Epoch 38/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.2426 - acc: 0.9134\n",
      "Epoch 39/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.0535 - acc: 0.9747\n",
      "Epoch 40/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.0216 - acc: 0.9928\n",
      "Epoch 41/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.0117 - acc: 0.9964\n",
      "Epoch 42/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.3401 - acc: 0.8989\n",
      "Epoch 43/60\n",
      "5/5 [==============================] - 11s 2s/step - loss: 0.0265 - acc: 0.9928\n",
      "Epoch 44/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.0460 - acc: 0.9819\n",
      "Epoch 45/60\n",
      "5/5 [==============================] - 11s 2s/step - loss: 0.0116 - acc: 1.0000\n",
      "Epoch 46/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.0505 - acc: 0.9819\n",
      "Epoch 47/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.0160 - acc: 0.9928\n",
      "Epoch 48/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.2693 - acc: 0.9097\n",
      "Epoch 49/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.0407 - acc: 0.9856\n",
      "Epoch 50/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.0075 - acc: 0.9964\n",
      "Epoch 51/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.0816 - acc: 0.9639\n",
      "Epoch 52/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.0253 - acc: 0.9892\n",
      "Epoch 53/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.0266 - acc: 0.9928\n",
      "Epoch 54/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.0500 - acc: 0.9783\n",
      "Epoch 55/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.2044 - acc: 0.9314\n",
      "Epoch 56/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.0178 - acc: 0.9964\n",
      "Epoch 57/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.0256 - acc: 0.9928\n",
      "Epoch 58/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.0158 - acc: 0.9928\n",
      "Epoch 59/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.0638 - acc: 0.9819\n",
      "Epoch 60/60\n",
      "5/5 [==============================] - 12s 2s/step - loss: 0.0059 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "inc_history = model2.fit_generator(train_image_gen, epochs = 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 7s 1s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,\n",
       "       1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "       1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
       "       1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = np.argmax(model2.predict(train_image_gen), axis = 1)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.51      0.52       142\n",
      "           1       0.50      0.50      0.50       135\n",
      "\n",
      "    accuracy                           0.51       277\n",
      "   macro avg       0.51      0.51      0.51       277\n",
      "weighted avg       0.51      0.51      0.51       277\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(train_image_gen.classes,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Result CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Image_1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Image_2.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Image_3.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Image_4.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Image_5.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      filename\n",
       "0  Image_1.jpg\n",
       "1  Image_2.jpg\n",
       "2  Image_3.jpg\n",
       "3  Image_4.jpg\n",
       "4  Image_5.jpg"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_csv = pd.read_csv(\"../img/BeesAndAnts/Testing_set.csv\")\n",
    "test_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 3s 1s/step\n"
     ]
    }
   ],
   "source": [
    "test_csv.label = np.argmax(model1.predict(test_image_gen), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv.to_csv(\"test_result_1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 3s 1s/step\n"
     ]
    }
   ],
   "source": [
    "test_csv.label = np.argmax(model2.predict(test_image_gen), axis = 1)\n",
    "test_csv.to_csv(\"test_result_2.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
